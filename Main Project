

## LightGBM
LightGBM stands for light gradient boosting. This algorithm has multiple advantages. LightGBM has been proven more accurate and less time-consuming in model training compared to other boosting algorithm. It is more compatible with large data set with multiple feature.

Like XGBoost, lightGBM operates upon maximizing information gain. However, xgboost grows trees depth-wise, while LightGBM grows trees leaf-wise.

Traditional gradient boosting decision tree need to scan all the data instances for every feature to estimate the best split points, making processing big data time-consuming and also it requires much memory. Instead, LightGBM uses histogram-based algorithm. It puts continuous feature values into discrete bins, thus increasing the training speed and model efficiency.

To further increase the speed of model training, Gradient-based One-Side Sampling(GOSS) method was introduced. LightGBM reduces the complexity of training procedure by keeping the data points with greater gradients and performs random sampling on data points with smaller gradients. To conpensate that part of data loss, GOSS introduces a constant multiplier for the data instances with small gradients.

This algorithm introduced another method called exclusive feature bundling to elaborate its compatibility with big data sets by reducing the number features. This method is based on the observation that for big data, the features are usually very sparse that they are mutually exclusive. Thus, we can bundle these mutually exclusive features together. The bundle will be treated as a single data point without any loss of information.

Now, let's test the lightGBM algorithm provided by Microsoft on our concrete dataset.
